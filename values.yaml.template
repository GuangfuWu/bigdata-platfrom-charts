serviceAccount:
  create>: false

postgresql-interval:
  enabled: true
  auth:
    postgresPassword: "Root@2022"
    database: "training"
    username: "training"
    password: "Training@2022"
  primary:
    initdb:
      user: postgres
      password: Root@2022
      scripts:
        db_init.sql: |
          CREATE DATABASE airflow;
          CREATE ROLE airflow WITH ENCRYPTED PASSWORD 'Airflow@2022' LOGIN;
          GRANT ALL PRIVILEGES ON DATABASE airflow TO airflow;
          
          CREATE DATABASE dolphinscheduler;
          CREATE ROLE dolphinscheduler WITH ENCRYPTED PASSWORD 'Dolphinscheduler@2022' LOGIN;
          GRANT ALL PRIVILEGES ON DATABASE dolphinscheduler TO dolphinscheduler;
          
          CREATE DATABASE metastore;
          CREATE ROLE hive WITH ENCRYPTED PASSWORD 'Hive@2022' LOGIN;
          GRANT ALL PRIVILEGES ON DATABASE metastore TO hive;
          
          CREATE DATABASE hue;
          CREATE ROLE hue WITH ENCRYPTED PASSWORD 'Hue@2022' LOGIN;
          GRANT ALL PRIVILEGES ON DATABASE hue TO hue;
          
          CREATE DATABASE superset;
          CREATE ROLE superset WITH ENCRYPTED PASSWORD 'Superset@2022' LOGIN;
          GRANT ALL PRIVILEGES ON DATABASE superset TO superset;
          
          flush privileges;
redis:
  enabled: true
  architecture: standalone
  auth:
    ## Enable password authentication
    enabled: false

openldap:
  enabled: true
  adminPassword: Root@2022
  configPassword: Root@2022
  env:
    LDAP_ORGANISATION: "Training Inc."
    LDAP_DOMAIN: "demo.com"
    LDAP_TLS: "false"
    LDAP_TLS_VERIFY_CLIENT: "nerver"
  customLdifFiles:
    01-default-users.ldif: |-
      # Entry 2: ou=databu,dc=demo,dc=com
      dn: ou=databu,dc=demo,dc=com
      objectclass: organizationalUnit
      objectclass: top
      ou: databu
      
      # Entry 3: cn=yhj,ou=databu,dc=demo,dc=com
      dn: cn=yhj,ou=databu,dc=demo,dc=com
      cn: yhj
      gidnumber: 500
      givenname: y
      homedirectory: /home/users/yhj
      loginshell: /bin/bash
      objectclass: inetOrgPerson
      objectclass: posixAccount
      objectclass: top
      sn: hj
      uid: yhj
      uidnumber: 1001
      userpassword: {MD5}lOjN5GErP9OQZ31C57IgAg==
      
      # Entry 4: cn=zhy,ou=databu,dc=demo,dc=com
      dn: cn=zhy,ou=databu,dc=demo,dc=com
      cn: zhy
      gidnumber: 500
      givenname: z
      homedirectory: /home/users/zhy
      loginshell: /bin/bash
      objectclass: inetOrgPerson
      objectclass: posixAccount
      objectclass: top
      sn: hy
      uid: zhy
      uidnumber: 1007
      userpassword: {MD5}lOjN5GErP9OQZ31C57IgAg==
      
      # Entry 5: cn=zjh,ou=databu,dc=demo,dc=com
      dn: cn=zjh,ou=databu,dc=demo,dc=com
      cn: zjh
      employeetype: Admin
      gidnumber: 500
      givenname: z
      homedirectory: /home/users/zjh
      loginshell: /bin/bash
      objectclass: inetOrgPerson
      objectclass: posixAccount
      objectclass: top
      sn: jh
      uid: zjh
      uidnumber: 1000
      userpassword: {MD5}lOjN5GErP9OQZ31C57IgAg==

  phpldapadmin:
    ingress:
      enabled: true
      hostname: phpldapadmin.demo.com

## @section Hadoop parameters
hadoop:
  ## @section hadoop.enabled enable the hadoop components on the platfrom
  enabled: true
  conf:
    ## @param hadoop.conf.coreSite will append the key and value to the core-site.xml file
    coreSite:
      # defined the Unix user[hue] that will run the hue supervisor server as a proxyuser. ref: https://docs.gethue.com/administrator/configuration/connectors/#hdfs
      hadoop.proxyuser.hue.hosts: "*"
      hadoop.proxyuser.hue.groups: "*"
      # defined the Unix user[httpfs] that will run the HttpFS server as a proxyuser. ref: https://hadoop.apache.org/docs/stable/hadoop-hdfs-httpfs/ServerSetup.html
      hadoop.proxyuser.httpfs.hosts: "*"
      hadoop.proxyuser.httpfs.groups: "*"
      # defined the Unix user[hive] that will run the HiveServer2 server as a proxyuser. ref: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/Superusers.html
      hadoop.proxyuser.hive.hosts: "*"
      hadoop.proxyuser.hive.groups: "*"
    hdfsSite:
      dfs.permissions.enabled: false
      dfs.webhdfs.enable: true
      dfs.replication: 3
    httpfsSite:
      # Hue HttpFS proxy user setting. ref: https://docs.gethue.com/administrator/configuration/connectors/#hdfs
      httpfs.proxyuser.hue.hosts: "*"
      httpfs.proxyuser.hue.groups: "*"
  ingress:
    nameNode:
      enabled: true
      hosts:
        - hdfs.demo.com
    resourcemanager:
      enabled: true
      hosts:
        - resourcemanager.demo.com
  persistence:
    nameNode:
      enabled: true
    dataNode:
      enabled: true
hive:
  enabled: true
  hive-metastore:
    enabled: true
    postgresql:
      enabled: false
      host: '{{ template "platform.postgresql-interval" . }}'
      port: 5432
      auth:
        database: "metastore"
        username: "hive"
        password: "Hive@2022"
  conf:
    hiveSite:
      hive.metastore.warehouse.dir: hdfs://$releaseName-hadoop-namenode:9820/user/hive/warehouse
      hive.metastore.schema.verification: false
    hadoopConfigMap: $releaseName-hadoop-hadoop
spark:
  enabled: true
  ingress:
    enabled: true
    hostname: spark.demo.com

airflow:
  enabled: true
  airflow:
    image:
      repository: 5200710/airflow
      tag: 2.3.0
      pullPolicy: IfNotPresent
    webserverSecretKey: 8e74ab588296bd461aff914a7af2755d
  web:
    webserverConfig:
      stringOverride: |
        from flask_appbuilder.security.manager import AUTH_LDAP

        AUTH_TYPE = AUTH_LDAP
        AUTH_LDAP_SERVER = "ldap://$releaseName-openldap:389"
        AUTH_LDAP_USE_TLS = False

        # registration configs
        AUTH_USER_REGISTRATION = True  # allow users who are not already in the FAB DB
        AUTH_USER_REGISTRATION_ROLE = "Admin"  # this role will be given in addition to any AUTH_ROLES_MAPPING
        AUTH_LDAP_FIRSTNAME_FIELD = "givenName"
        AUTH_LDAP_LASTNAME_FIELD = "sn"
        AUTH_LDAP_EMAIL_FIELD = "mail"  # if null in LDAP, email is set to: "{username}@email.notfound"

        # search configs
        AUTH_LDAP_SEARCH = "ou=databu,dc=demo,dc=com"  # the LDAP search base
        AUTH_LDAP_UID_FIELD = "uid"  # the username field
        AUTH_LDAP_BIND_USER = "cn=admin,dc=demo,dc=com"  # the special bind username for search
        AUTH_LDAP_BIND_PASSWORD = "Root@2022"  # the special bind password for search

        # a mapping from LDAP DN to a list of FAB roles
        AUTH_ROLES_MAPPING = {
            "cn=zhy,ou=databu,dc=demo,dc=com": ["User"],
            "cn=zjh,ou=databu,dc=demo,dc=com": ["Admin"],
        }

        # the LDAP user attribute which has their role DNs
        AUTH_LDAP_GROUP_FIELD = "memberOf"

        # if we should replace ALL the user's roles each login, or only on registration
        AUTH_ROLES_SYNC_AT_LOGIN = True

        # force users to re-auth after 30min of inactivity (to keep roles in sync)
        PERMANENT_SESSION_LIFETIME = 1800

  dags:
    gitSync:
      enabled: true
      repo: https://github.com/apache/airflow.git
      branch: v2-2-stable
      repoSubPath: "tests/dags"
  postgresql:
    enabled: false
  externalDatabase:
    host: $releaseName-postgresql-interval
    password: "Airflow@2022"
  redis:
    enabled: false
  externalRedis:
    host: $releaseName-redis-master
    port: 6379
  ingress:
    enabled: true
    web:
      host: airflow.demo.com
    flower:
      host: flower.demo.com

hue:
  ingress:
    enabled: true
    hosts:
      - hue.demo.com
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 2048m
  enabled: true
  postgresql:
    enabled: false
    host: '{{ template "platform.postgresql-interval" . }}'
    auth:
      password: "Hue@2022"
  hue:
    replicas: 1
    interpreters: |-
      [[[postgresql]]]
      name = postgresql
      interface=sqlalchemy
      options='{"url": "postgresql://training:Training@2022@$releaseName-postgresql-interval:5432/training"}'
    zz_hue_ini: |
      [desktop]
      secret_key=hue123
      # ref: https://gethue.com/mini-how-to-disabling-some-apps-from-showing-up/
      app_blacklist=spark,zookeeper,hbase,impala,search,pig,sqoop,security,oozie,jobsub,jobbrowser
      django_debug_mode=false
      gunicorn_work_class=sync
      enable_prometheus=true

      [[task_server]]
      enabled=false
      broker_url=redis://redis:6379/0
      result_cache='{"BACKEND": "django_redis.cache.RedisCache", "LOCATION": "redis://redis:6379/0", "OPTIONS": {"CLIENT_CLASS": "django_redis.client.DefaultClient"},"KEY_PREFIX": "queries"}'
      celery_result_backend=redis://redis:6379/0

      [[custom]]
      [[auth]]
      backend=desktop.auth.backend.LdapBackend,desktop.auth.backend.AllowFirstUserDjangoBackend

      [[ldap]]
      ldap_url=ldap://{{ template "platform.openldap" . }}:389
      search_bind_authentication=true
      use_start_tls=false
      create_users_on_login=true
      base_dn="ou=databu,dc=demo,dc=com"
      bind_dn="cn=admin,dc=demo,dc=com"
      bind_password=Root@2022
      test_ldap_user="cn=admin,dc=demo,dc=com"
      test_ldap_group="cn=openldap,dc=demo,dc=com"

      [[[users]]]
      user_filter="objectClass=posixAccount"
      user_name_attr="uid"

      [[[groups]]]
      group_filter="objectClass=posixGroup"
      group_name_attr="cn"
      group_member_attr="memberUid"

      [beeswax]
      # Host where HiveServer2 is running.
      hive_server_host=$releaseName-hive-hiveserver
      # Port where HiveServer2 Thrift server runs on.
      hive_server_port=10000
      thrift_version=7

      [notebook]
      [[interpreters]]
      [[[hive]]]
      name=Hive
      interface=hiveserver2

      [hadoop]
      [[hdfs_clusters]]
      [[[default]]]
      fs_defaultfs=hdfs://{{ template "platform.hadoop" . }}:9820
      webhdfs_url=http://{{ template "platform.hadoop" . }}-httpfs:14000/webhdfs/v1
      
      # Configuration for YARN (MR2)
      # ------------------------------------------------------------------------
      [[yarn_clusters]]
      [[[default]]]
      resourcemanager_host={{ template "platform.hadoop" . }}-resourcemanager-hl
      resourcemanager_api_url=http://{{ template "platform.hadoop" . }}-resourcemanager-hl:8088/
      resourcemanager_port=8032
      history_server_api_url=http://{{ template "platform.hadoop" . }}-historyserver-hl:19888/
      spark_history_server_url=http://{{ template "platform.hadoop" . }}-spark-master-svc:18080
superset:
  image:
    tag: 2.0.0
  ingress:
    enabled: true
    hosts:
      - superset.demo.com
  enabled: true
  postgresql:
    enabled: false
  redis:
    enabled: false
  init:
    adminUser:
      password: Root@2022
  supersetNode:
    connections:
      redis_host: "$releaseName-redis-headless"
      # You need to change below configuration incase bringing own PostgresSQL instance and also set postgresql.enabled:false
      db_host: '{{ template "platform.postgresql-interval" . }}'
      db_pass: Superset@2022
  bootstrapScript: |
    #!/bin/bash
    apt-get update -y && apt-get install -y libsasl2-dev python-dev libldap2-dev libssl-dev && \
    rm -rf /var/lib/apt/lists/* && \
    pip install \
      psycopg2-binary==2.9.1 \
      python-ldap==3.4.0 \
      PyHive==0.6.5 \
      mysqlclient==2.1.1 \
      redis==3.5.3 && \
    if [ ! -f ~/bootstrap ]; then echo "Running Superset with uid {{ .Values.runAsUser }}" > ~/bootstrap; fi
  # A dictionary of overrides to append at the end of superset_config.py - the name does not matter
  configOverrides:
    # Generate your own secret key for encryption. Use openssl rand -base64 42 to generate a good key
    secret: |
      SECRET_KEY = 'sOpgvM1hGAohEXS2fafKH3IS8gUAaKHU4mYFTr7h1FsM9gT2tp7N6v1S'
    enable_oauth: |
      # 引入AUTH_LDAP
      from flask_appbuilder.security.manager import AUTH_DB,AUTH_LDAP

      # 修改LDAP配置
      AUTH_TYPE = AUTH_LDAP

      # Uncomment to setup Full admin role name
      AUTH_ROLE_ADMIN = 'admin'

      # 打开自注册配置
      # Will allow user self registration
      AUTH_USER_REGISTRATION = True

      # 打开此注释，并改为admin角色
      # The default user self registration role
      AUTH_USER_REGISTRATION_ROLE = "admin"

      # When using LDAP Auth, setup the ldap server
      AUTH_LDAP_SERVER = 'ldap://{{ template "platform.openldap" . }}:389'
      AUTH_LDAP_SEARCH = "ou=databu,dc=demo,dc=com"
      AUTH_LDAP_UID_FIELD = "cn"

      # 绑定某个初始账号，我这里用的app组织的账号，跟上面People不是同一个ou,无所谓，能登录就行，主要是不希望配置个人账号到应用里。
      AUTH_LDAP_BIND_USER = "cn=admin,dc=demo,dc=com"
      AUTH_LDAP_BIND_PASSWORD = "Root@2022"
dolphinscheduler:
  enabled: true
  postgresql:
    enabled: false
  conf:
    common:
      resource.storage.type: HDFS
      fs.defaultFS: hdfs://$releaseName-hadoop-namenode:9820
  externalDatabase:
    type: "postgresql"
    host: "$releaseName-postgresql-interval"
    port: "5432"
    username: "dolphinscheduler"
    password: "Dolphinscheduler@2022"
    database: "dolphinscheduler"
    params: "characterEncoding=utf8"
  common:
    configmap:
      RESOURCE_STORAGE_TYPE: "HDFS"
      FS_DEFAULT_FS: "hdfs://$releaseName-hadoop-namenode:9820"
    sharedStoragePersistence:
      enabled: true
      storageClassName: rook-cephfs
  ingress:
    enabled: true
    host: "dolphinscheduler.demo.com"
    path: "/"
    annotations:
      nginx.ingress.kubernetes.io/proxy-body-size: 2048m